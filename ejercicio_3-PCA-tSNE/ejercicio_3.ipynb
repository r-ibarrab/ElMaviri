{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 3: Reducción de dimensionalidad\n",
    "En este ejercicio exploraremos el uso de PCA labels t-SNE usando las librerias de scikit-learn. \n",
    "Específicamente, reduciremos la dimensionalidad de los datos para analizar cuando un tumor es benigno o maligno.\n",
    "\n",
    "Para entender el uso de estos metodos con scikit-learn, puedes consultar la documentación oficial de scikit-learn para [PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)\n",
    "\n",
    "En este ejercicio:\n",
    "\n",
    "1. Utilizarás y explorarás un dataset de clasificación de tumores\n",
    "2. Implementarás la solución analítica de PCA\n",
    "3. Compararás tu solución con la implementación de scikit learn\n",
    "4. Utilizarás PCA para poder visualizar tus datos de 30 variables en 2 dimensiones\n",
    "5. Analizarás el uso de PCA para visualizar en 2 dimensiones, la predicción de un modelo de clasificación de 30 dimensiones\n",
    "\n",
    "\n",
    "## 1. Explorando tus datos\n",
    "En las siguientes celdas, cargamos el dataset que utilizamos en el ejercicio de regresión logística. Este es un conjunto de datos de clasificación, donde cada punto tiene 30 atributos o variables de entrada. Corre la siguiente celda para cargar el dataset y separar los datos en entrenamiento y prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos originales de dimensionalidad (426, 30)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# En esta ocasión leeremos el dataset como un dataframe de pandas\n",
    "dataset = load_breast_cancer(as_frame=True)\n",
    "data_train, data_test, target_train, target_test = train_test_split(\n",
    "    dataset['data'], \n",
    "    dataset['target'], \n",
    "    test_size=0.25,\n",
    "    random_state=10\n",
    ")\n",
    "print(f\"Datos originales de dimensionalidad {data_train.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos imprimir los primeros tres renglones de nuestros datos. Esto nos sevirá para identificar como son los datos originales antes de aplicar la reducción de dimensionalidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caracteristicas\n",
      "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
      "327        12.03         17.93           76.09      446.0          0.07683   \n",
      "60         10.17         14.88           64.55      311.9          0.11340   \n",
      "260        20.31         27.06          132.90     1288.0          0.10000   \n",
      "\n",
      "     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
      "327           0.03892        0.001546             0.005592         0.1382   \n",
      "60            0.08061        0.010840             0.012900         0.2743   \n",
      "260           0.10880        0.151900             0.093330         0.1814   \n",
      "\n",
      "     mean fractal dimension  ...  worst radius  worst texture  \\\n",
      "327                 0.06070  ...         13.07          22.25   \n",
      "60                  0.06960  ...         11.02          17.45   \n",
      "260                 0.05572  ...         24.33          39.16   \n",
      "\n",
      "     worst perimeter  worst area  worst smoothness  worst compactness  \\\n",
      "327            82.74       523.4            0.1013            0.07390   \n",
      "60             69.86       368.6            0.1275            0.09866   \n",
      "260           162.30      1844.0            0.1522            0.29450   \n",
      "\n",
      "     worst concavity  worst concave points  worst symmetry  \\\n",
      "327         0.007732               0.02796          0.2171   \n",
      "60          0.021680               0.02579          0.3557   \n",
      "260         0.378800               0.16970          0.3151   \n",
      "\n",
      "     worst fractal dimension  \n",
      "327                  0.07037  \n",
      "60                   0.08020  \n",
      "260                  0.07999  \n",
      "\n",
      "[3 rows x 30 columns]\n"
     ]
    }
   ],
   "source": [
    "# Visualizar las primeras tres filas\n",
    "print(\"Caracteristicas\")\n",
    "print(data_train.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Etiquetas\n",
      "327    1\n",
      "60     1\n",
      "260    0\n",
      "Name: target, dtype: int32\n"
     ]
    }
   ],
   "source": [
    "print(\"Etiquetas\")\n",
    "print(target_train.head(3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implementación de PCA\n",
    "Para entender los conceptos vistos en clase, comenzaremos utilizando solo 2 variables de cada muestra. Es decir, aplicaremos PCA a los datos anteriores para analizar la relevancia de los atributos según las asunciones de PCA.\n",
    "1. Aprenderemos como trasladar los puntos al centro para que tengan una promedio de 0\n",
    "2. Obtendremos la matriz de covarianza de nuestros datos\n",
    "3. Veremos los eigenvalues en la gráfica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2c2cff53a00>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Nos quedamos solo con los componentes \"mean radius\" y \"mean texture\"\n",
    "X = data_train[[\"mean radius\", \"mean texture\"]].to_numpy()\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(4, 4))\n",
    "ax.set_title(\"Muestras originales\")\n",
    "ax.axis('equal')\n",
    "\n",
    "\n",
    "# En el siguiente foorloop graficamos de manera separada los puntos de distintas clases\n",
    "# En este contexto, los puntos de diferentes clases se muestran en un color distinto.\n",
    "for c in range(2):\n",
    "    # TODO: Selecciona los valores de X que corresponden a la clase c\n",
    "    # es decir, X cuando la etiqueta target_train == c\n",
    "    # === Start solution ==== #\n",
    "    class_datapoints = X[...]\n",
    "    # === End solution ==== #\n",
    "    ax.scatter(\n",
    "        class_datapoints[:, 0],  # Todos los datos de clase c, columna de atributo 1\n",
    "        class_datapoints[:, 1],  # Todos los datos de clase c, columna de atributo 2\n",
    "        label = f\"class {c}\"\n",
    "    )\n",
    "ax.set_xlabel(\"Mean radius\")\n",
    "ax.set_ylabel(\"Mean texture\")\n",
    "# Imprimirá el origen\n",
    "ax.scatter(0, 0, c=\"black\", marker=\"+\")\n",
    "ax.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Datos con media cero\n",
    "En la imagen observamos que se encuentran desplazados del origen (representado con un '+' negro). Recuerda que PCA asume que los datos tienen una media de cero. Entonces, para poder aplicar la proyección de PCA primero necesitamos restar la media de nuestros puntos para moverlos alrededor del origen. Como nuestros datos tienen dimensionalidad, nuestra media tambien la tendrá.\n",
    "\n",
    "$X \\in \\mathbb{R}^{N \\times D}$\n",
    "\n",
    "$\\mu \\in \\mathbb{R}^{D}$\n",
    "\n",
    "$\\mu  = \\left[\\begin{matrix} \n",
    "                \\frac{1}{N} \\sum_{i=1}^{N}{X[i, 0]} \\\\\n",
    "                \\frac{1}{N} \\sum_{i=1}^{N}{X[i, 1]}  \\\\\n",
    "                \\dots \\\\\n",
    "                \\frac{1}{N} \\sum_{i=1}^{N}{X[i, D]}  \\\\\n",
    "                \\end{matrix} \\right]$\n",
    "\n",
    "Completa el método siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_zero_mean_data(X):\n",
    "    # TODO: Restale a X el valor promedio de todos los puntos\n",
    "    # Recuerda que X.shape = (N, 2) por lo que \n",
    "    # a cada punto N tienes que restarle el promedio de X[:, 0] a la primera dimensión\n",
    "    # y el promedio de X[:, 1] a la segunda dimensión.\n",
    "    # Consejo: Investiga el uso del atributo 'axis' para la función np.mean\n",
    "    # ======== START SOLUTION =======\n",
    "    X_proc = ...\n",
    "    # ========= END SOLUTION ======== \n",
    "    return X_proc\n",
    "\n",
    "# Asegúrate de que el valor que imprima sea de (N, D) = (426, 2)\n",
    "# Y que los datos de media cero esten asignados a X_proc\n",
    "X = data_train[[\"mean radius\", \"mean texture\"]].to_numpy()\n",
    "X_proc = get_zero_mean_data(X)\n",
    "print(\"media shape\", X_proc.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez calculado lo anterior, podemos graficar nuestros nuevos datos con media cero. Corre la siguiente celda para visualizar tus nuevos datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============= Muestras originales ==============\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4))\n",
    "ax1.set_title(\"Muestras originales\")\n",
    "ax1.axis('equal')\n",
    "for c in range(2):\n",
    "    class_datapoints = X[target_train == c]\n",
    "    ax1.scatter(\n",
    "        class_datapoints[:, 0], \n",
    "        class_datapoints[:, 1], \n",
    "        label = f\"class {c}\"\n",
    "    )\n",
    "ax1.set_xlabel(\"Mean radius\")\n",
    "ax1.set_ylabel(\"Mean texture\")\n",
    "ax1.scatter(0, 0, c=\"black\", marker=\"+\")\n",
    "ax1.legend()\n",
    "\n",
    "\n",
    "# Nuevos datos\n",
    "ax2.set_title(\"Muestras con media de 0\")\n",
    "ax2.axis('equal')\n",
    "ax2.scatter(0, 0, c=\"black\", marker=\"+\")\n",
    "# Grafica los datos y veras la figura desplazada hacia el origen\n",
    "for c in range(2):\n",
    "    ax2.scatter(\n",
    "        X_proc[:, 0][target_train == c], \n",
    "        X_proc[:, 1][target_train == c], \n",
    "        label = f\"class {c}\"\n",
    "    )\n",
    "ax2.legend()\n",
    "# Imprimirá el origen\n",
    "ax2.scatter(0, 0, c=\"black\", marker=\"+\")\n",
    "ax2.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2.2-3 Direcciones de proyección y la matríz de covarianza\n",
    "En clase vimos que la solución de PCA está dada por los vectores propios de la matriz de covarianza. Ya que estós determinan la mejor dirección de proyección según las asunciones del método.\n",
    "\n",
    "En la siguiente celda, termina el método para calcular los valores y vectores propios de la matríz de covarianza.\n",
    "Recuerda que los vectores propios $v \\in \\mathbb{R}^{D \\times D}$ donde $v[:, i] \\forall i = {0,1, \\dots, D}$ corresponde al vector (columna) propio $v_i$. Asegúrate de que tu método regrese los vectores de dimensionalidad correcta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sorted_eigvectors(X):\n",
    "    '''\n",
    "        Calcula los vectores y valores propios de una matriz con media cero X\n",
    "        args:\n",
    "            - X  (np.ndarray): Matriz de datos con media cero de dimensionalidad N x D\n",
    "        returns:\n",
    "            - eigenvectors (np.ndarray): vectores propios de la matriz de cov. de X.  Dimensionalidad de D x D\n",
    "            - eigenvalues (np.ndarray, dtype=float): valores propios de la matriz de cov. de X en el mismo orden que los vectores propios\n",
    "    '''\n",
    "    # Estimaremos una matriz de covarianza para los puntos con\n",
    "    # promedio de 0 y desviacion estandar de 1\n",
    "    sample_cov = np.cov(X, rowvar=0)\n",
    "    # TODO: Calcula los valores y vectores propios de la matriz de covarianza\n",
    "    # Utiliza np.linalg.eig()\n",
    "    # =========== Start solution ==============\n",
    "    eigenvalues, eigenvectors = ...\n",
    "    # =========== End solution ================\n",
    "\n",
    "    # La sig. linea regresa los índices de los eigenvalues en orden del mayor al menor eigenvalue\n",
    "    # [::-1] invierte el orden (osea si argsort regresa del menor al mayor ...\n",
    "    # ... usar [::-1] hace que regrese de mayor a menor)\n",
    "    idx_ordered =  np.argsort(eigenvalues)[::-1]\n",
    "\n",
    "    # TODO: Ordena los eigenvalues y eigenvectors de mayor a menor eigenvalue\n",
    "    # Utiliza los índices previamente calculados\n",
    "    # regresa los eigenvectors y eigenvalues en el orden indicado\n",
    "    # =========== Start solution ==============\n",
    "\n",
    "    # =========== End solution ================\n",
    "    return eigenvectors, eigenvalues\n",
    "\n",
    "# Nos quedamos solo con los componentes \"mean radius\" y \"mean texture\"\n",
    "X = data_train[[\"mean radius\", \"mean texture\"]].to_numpy()\n",
    "X_proc = get_zero_mean_data(X)\n",
    "# Tomar los eigenvectors y eigenvalues de la matriz de covarianza\n",
    "eigvecs, eigvals = get_sorted_eigvectors(X)\n",
    "assert eigvecs.shape == (2,2), f\"eigvecs.shape de dimensionalidad {eigvecs.shape} debería ser (2x2)\"\n",
    "print(\"Vectores propios de dimensionalidad:\", eigvecs.shape)\n",
    "\n",
    "# TODO: Proyecta los datos de media cero a los primeros dos vectores propios\n",
    "# y asigna el resultado a X_proj\n",
    "# =========== Start solution ==============\n",
    "X_proj = ...\n",
    "# =========== End solution ================\n",
    "\n",
    "assert X_proj.shape == (len(X),2), f\"X_proj.shape de dimensionalidad {X_proj.shape} debería ser ({len(X)}x2)\"\n",
    "print(\"Proyección de dimensionalidad:\", X_proj.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corre la siguiente celda para visualizar los vectores propios sobrepuestos a tus datos originales y a los datos proyectados a la base dada por los mismos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(9, 4))\n",
    "# ============= Datos con media cero ==============\n",
    "ax1.set_title(\"Visualización de vectores propios\")\n",
    "ax1.axis('equal')\n",
    "# Graficamos los datos de nuevp\n",
    "for c in range(2):\n",
    "    ax1.scatter(\n",
    "        X_proc[:, 0][target_train == c], \n",
    "        X_proc[:, 1][target_train == c], \n",
    "        label = f\"class {c}\"\n",
    "    )\n",
    "# Graficamos los vectores sobrepuestos a los datos en el ax anterior\n",
    "for idx_col in range(eigvals.shape[0]):\n",
    "    norm_eig = np.sqrt(eigvals[idx_col])\n",
    "    start_point = [0, 0]\n",
    "    end_point = eigvecs[:, idx_col] * norm_eig\n",
    "    ax1.arrow(*start_point, *end_point, width = 0.03, head_width = 0.6)\n",
    "ax1.legend()\n",
    "ax1.scatter(0, 0, c=\"black\", marker=\"+\")\n",
    "\n",
    "# ============= Muestras proyectadas ==============\n",
    "ax2.set_title(\"Muestras proyectadas en eigenvectors\")\n",
    "ax2.axis('equal')\n",
    "ax2.scatter(0, 0, c=\"black\", marker=\"+\")\n",
    "for c in range(2):\n",
    "    ax2.scatter(\n",
    "        X_proj[:, 0][target_train == c], \n",
    "        X_proj[:, 1][target_train == c], \n",
    "        label = f\"class {c}\"\n",
    "    )\n",
    "# Graficamos los vectores sobrepuestos a los datos en el ax anterior\n",
    "end_pts = np.array([[1,0],[0,1]])\n",
    "for idx_col in range(eigvals.shape[0]):\n",
    "    norm_eig = np.sqrt(eigvals[idx_col])\n",
    "    start_point = [0, 0]\n",
    "    end_point = end_pts[idx_col] * norm_eig\n",
    "    ax2.arrow(*start_point, *end_point, width = 0.03, head_width = 0.6)\n",
    "ax2.scatter(0, 0, c=\"black\", marker=\"+\")\n",
    "ax2.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Comparación con la librería de scikit learn\n",
    "En la siguiente sección compararemos tu implementación de PCA con la de scikit learn. Primero analizaremos proyectar datos de dos dimensiones en 1, y después proyectaremos nuestros datos de 10 variables a 2 para poder visualizarlos.\n",
    "\n",
    "### Usando PCA para reducir 2 dimensiones a 1\n",
    "En las siguientes celdas filtraremos los datos para inicialmente trabajar con 2 dimensiones. Después utilizaremos los métodos que anteriormente definiste (`get_zero_mean_data` & `get_sorted_eigenvectors`) para comparar tu implementación de PCA con la de scikit-learn.\n",
    "\n",
    "La siguiente celda define un método que normaliza los datos (resta la media y escala los datos) para poder proyectarlos a los eigenvectors calculados por `get_sorted_eigenvectors`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_data(X):\n",
    "    mean = X.mean(0)\n",
    "    std = X.std(0)\n",
    "    X_proc = (X - mean)\n",
    "    new_data = X_proc / std\n",
    "    return new_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corre la siguiente celda, analiza la imagen y responde:\n",
    "\n",
    "\\# TODO:\n",
    "Para la tarea de clasificación ¿Cual vector propio crees que te permita separar mejor los datos al proyectarse en el? Explica tu intuición."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat1, feat2 = \"mean radius\", \"mean compactness\"\n",
    "labels = target_train\n",
    "X = data_train[[feat1, feat2]].to_numpy()\n",
    "X = norm_data(X)\n",
    "print(X.shape)\n",
    "\n",
    "eigvecs, eigvals = get_sorted_eigvectors(X)\n",
    "print(eigvecs.shape)\n",
    "\n",
    "# ========== Graficas ===============\n",
    "fig, ax = plt.subplots(1, 1, figsize=(3, 3))\n",
    "# Espacio original\n",
    "for c in range(2):\n",
    "    ax.scatter(\n",
    "        X[:, 0][labels == c], \n",
    "        X[:, 1][labels == c], \n",
    "        label = f\"class {c}\"\n",
    "    )\n",
    "\n",
    "# Graficamos los vectores sobrepuestos a los datos en el ax anterior\n",
    "for idx_col in range(eigvals.shape[0]):\n",
    "    norm_eig = np.sqrt(eigvals[idx_col])\n",
    "    start_point = np.mean(X, axis=0)\n",
    "    end_point = eigvecs[:, idx_col] * norm_eig\n",
    "    ax.arrow(*start_point, *end_point, width = 0.03, head_width = 0.3)\n",
    "ax.axis('equal')\n",
    "ax.set_title('Datos con media cero')\n",
    "ax.set_xlabel(feat1)\n",
    "ax.set_ylabel(feat2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Proyección a una dimensión\n",
    "Ahora que has visto tus datos considerando solo dos variables distintas. Vamos a proyectar los datos a ambos vectores propios para visualizar como se verían en 1 sola dimensión. \n",
    "\n",
    "Esto nos permite para analizar que proyección permite mejor separabilidad de los datos para la tarea de clasificación. Recuerda que los vectores propios $v \\in \\mathbb{R}^{D \\times D}$ donde $v[:, i] \\forall i = \\{0,1, \\dots, D\\}$ corresponde al vector (columna) propio $v_i$.\n",
    "\n",
    "\\#TODO:\n",
    "\n",
    "Corre las siguientes dos celdas y responde:\n",
    "\n",
    "En este caso, ¿se cumple la asunción de PCA de que el primer componente principal mantiene la información más importante?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normed_data_train = norm_data(data_train)\n",
    "# Datos normalizados (media cero y misma escala)\n",
    "X = normed_data_train[[feat1, feat2]].to_numpy()\n",
    "eigvecs, eigvals = get_sorted_eigvectors(X)\n",
    "\n",
    "# ========= TU Solución ============#\n",
    "# TODO: Proyecta los datos al primer componente principal eigvecs[:,0]\n",
    "# =========== Start solution ==============\n",
    "X_proj1 = ...\n",
    "\n",
    "# TODO: Proyecta los datos al segundo componente principal eigvecs[:,1]\n",
    "X_proj2 = ...\n",
    "# =========== End solution ================= \n",
    "\n",
    "\n",
    "# ========= Solución de scikit learn ============#\n",
    "from sklearn.decomposition import PCA\n",
    "# TODO: Utiliza la libreria de scikit-learn para reducir X a 1 dimensión\n",
    "# Invesitga PCA en scikit learn. Asigna el resultado a reduced_x\n",
    "# =========== Start solution ==============\n",
    "reduced_x = ...\n",
    "# =========== End solution ================= "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recuerda siempre correr la celda anterior antes de correr esta\n",
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(15, 3))\n",
    "# Espacio original\n",
    "for c in range(2):\n",
    "    ax1.scatter(\n",
    "        X[:, 0][labels == c], \n",
    "        X[:, 1][labels == c], \n",
    "        label = f\"class {c}\"\n",
    "    )\n",
    "\n",
    "# Graficamos los vectores sobrepuestos a los datos en el ax anterior\n",
    "for idx_col in range(eigvals.shape[0]):\n",
    "    norm_eig = np.sqrt(eigvals[idx_col])\n",
    "    start_point = np.mean(X, axis=0)\n",
    "    end_point = eigvecs[:, idx_col] * norm_eig\n",
    "    ax1.arrow(*start_point, *end_point, width = 0.03, head_width = 0.3)\n",
    "ax1.axis('equal')\n",
    "ax1.set_title('Datos con media cero')\n",
    "ax1.set_xlabel(feat1)\n",
    "ax1.set_ylabel(feat2)\n",
    "\n",
    "for c in range(2):\n",
    "    class_data = X_proj1[labels == c]\n",
    "    N_samples = len(class_data)\n",
    "    ax2.scatter(\n",
    "        class_data, \n",
    "        np.zeros(N_samples), \n",
    "        label = f\"class {c}\"\n",
    "    )\n",
    "ax2.set_title('Proyección al 1er CP')\n",
    "\n",
    "for c in range(2):\n",
    "    class_data = X_proj2[labels == c]\n",
    "    N_samples = len(class_data)\n",
    "    ax3.scatter(\n",
    "        class_data, \n",
    "        np.zeros(N_samples), \n",
    "        label = f\"class {c}\"\n",
    "    )\n",
    "ax3.set_title('Proyección al 2do CP')\n",
    "\n",
    "\n",
    "## Solución de scikit-learn (1 dim)\n",
    "# plot reduced data\n",
    "for c in range(2):\n",
    "    class_idx = np.where(target_train == c)[0]\n",
    "    N_samples = len(class_idx)\n",
    "    ax4.scatter(\n",
    "        reduced_x[class_idx][:,0],\n",
    "        np.zeros(N_samples),\n",
    "        label = f\"class {c}\"\n",
    "    )\n",
    "ax4.set_title('Solución de scikit-learn (1 dimensión)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que has visto como proyectar tus datos a distintos componentes principales. Implementa el método de mi_PCA usando las funciones anteriormente deifindas. Para ello proyecta los datos a los primeros `n_components` vectores propios y regresa el resultado de la proyección"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mi_PCA(data, n_components=1):\n",
    "    zero_mean = get_zero_mean_data(data)\n",
    "    eigvecs, eigvals = get_sorted_eigvectors(zero_mean)\n",
    "    # TODO: proyecta los datos a los primeros n_components vectores propios y regresa el resultado\n",
    "    # =========== Start solution ==============\n",
    "    x_proj = ...\n",
    "    # =========== End solution ================= \n",
    "    return x_proj\n",
    "\n",
    "# Prueba tu solución\n",
    "X = data_train.to_numpy()\n",
    "for output_dims in range(1,4):\n",
    "    reduced_x_mine = mi_PCA(X, output_dims)\n",
    "    assert reduced_x_mine.shape == (len(X),output_dims), f\"X_proj.shape de dimensionalidad {reduced_x_mine.shape} debería ser ({len(X)}x{output_dims})\"\n",
    "    print(f\"Mi solución: Datos originales {X.shape}, Datos proyectados para {output_dims} dims: {reduced_x_mine.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Usando PCA para reducir 30 dimensiones a 2\n",
    "\n",
    "En la sección anterior, filtramos nuestros datos originales para quedarnos solo con dos variables. En esta sección consideramos los datos en su estado original, es decir con las 30 variables de entrada. Por su puesto, es imposible visualizarlso de esta forma, por lo tanto usaremos PCA para reducir la dimensionalidad de los datos originales a solo 2 dimensiones para poder visualizarlos.\n",
    "\n",
    "Usando PCA podemos transformar un dataset de $D$ variables proyectándolos a los $K$ vectores propios con los mayores valores propios perdiendo la menor información posible. De esta manera PCA mapea.\n",
    "\n",
    "$X_{orig} \\in \\mathbb{R}^{N \\times D} \\mapsto X_{reduced} \\in \\mathbb{R}^{N \\times 2}$\n",
    "\n",
    "Podemos resolver problema utilizando tu implementación y las librerias de scikit-learn. Ambas deberían generar una distribución de datos muy similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_train.to_numpy()\n",
    "output_dims = 2\n",
    "# ============= TU solución ===============\n",
    "# TODO: Reduce los datos en X a output_dims usando tu implementación de mi_PCA(data, n_components=...)\n",
    "reduced_x_mine = ...\n",
    "# =========== End solution ================= \n",
    "print(f\"Mi solución: Datos originales {X.shape}, Datos proyectados {reduced_x_mine.shape}\")\n",
    "\n",
    "# ============= Solución  de scikit-learn ===============\n",
    "from sklearn.decomposition import PCA\n",
    "# TODO: Reduce los datos en X a output_dims usando la implementación de sciki-learn\n",
    "reduced_x_lib = ...\n",
    "# =========== End solution ================= \n",
    "print(f\"Scikit learn: Datos originales {X.shape}, Datos proyectados {reduced_x_lib.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficamos los datos de nuestra solución\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "for c in range(2):\n",
    "    ax1.scatter(\n",
    "        reduced_x_mine[:, 0][target_train == c], \n",
    "        reduced_x_mine[:, 1][target_train == c], \n",
    "        label = f\"class {c}\"\n",
    "    )\n",
    "ax1.set_title(\"Mi solución\")\n",
    "ax1.legend()\n",
    "\n",
    "# plot reduced data\n",
    "for c in range(2):\n",
    "    class_idx = np.where(target_train == c)\n",
    "    ax2.scatter(\n",
    "        reduced_x_lib[class_idx, 0],\n",
    "        reduced_x_lib[class_idx, 1],\n",
    "        label = f\"class {c}\"\n",
    "    )\n",
    "ax2.set_title(\"La solución de scikit-learn\")\n",
    "ax2.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Otros métodos de reducción de dimensionalidad\n",
    "\n",
    "Como PCA, existen otras alternativas para reducir la dimensionalidad de los datos. Una muy popular es el algoritmo de TSNE.\n",
    "Mientras que ambos métodos sirven el mismo propósito, se utilizan en situaciones distintas. PCA normalmente se utiliza cuando queremos reducir dimensionalidades muy altas, a unas pequeñas por ejemplo reducir de 100 a 50 o 10 dimensiones. \n",
    "\n",
    "Por otro lado TSNE funciona bien cuando tenemos una dimensionalidad relativamente baja (entre 20 y 5 por ejemplo) y queremos reducirla a una más pequeña para poder visualizar nuestros datos (por ejemplo 3,2 o 1).\n",
    "\n",
    "TSNE tiene un hiperparámetro adicional a la cantidad de componentes de salida, llamado \"perplexidad\". Si te interesa conocer más sobre TSNE puedes revisar la [documentación de sci-kit learn](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html).\n",
    "\n",
    "Corre la siguiente celda para visualizar como TSNE reduce las 30 dimensiones de nuestros datos originales a 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "# Convertimos nuestros datos de entrenamiento a un arreglo de numpy\n",
    "\n",
    "# Nuestra matriz de datos tiene 712 datos con 8 variables.\n",
    "X = data_train.to_numpy()\n",
    "tsne = TSNE(n_components=2, perplexity=50)\n",
    "reduced_x = tsne.fit_transform(X)\n",
    "print(reduced_x.shape)\n",
    "\n",
    "# plot reduced data\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "for c in range(2):\n",
    "    class_idx = np.where(target_train == c)\n",
    "    ax.scatter(\n",
    "        reduced_x[class_idx, 0],\n",
    "        reduced_x[class_idx,1], \n",
    "        label = f\"class {c}\"\n",
    "    )\n",
    "ax.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Reducción de dimensionalidad aplicado a tareas adicionales\n",
    "PCA normalmente se utiliza cuando queremos visualizar lo que ha aprendido nuestro algoritmo para entradas de alta dimensionalidad. Tenemos dos opciones\n",
    "\n",
    "1. Podemos entrenar con el dataset reducido un algoritmo de clasificación, como regresión logística.\n",
    "2. Podemos entrenar regresión logística en alta dimensionalidad y usar PCA para reducir la dimensionalidad y visualizar el resultado.\n",
    "\n",
    "Corre las siguientes celdas para observar como funciona (:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Podemos utilizar diferentes algoritmos de reducción de datos como TSNE\n",
    "# Aquí definimos un método para seleccionar que algoritmo usar\n",
    "# para reducir la dimensionalidad de nuestros datos\n",
    "def reduce_data(data, method='PCA', output_dims=2):\n",
    "    if method == 'TSNE':\n",
    "        tsne = TSNE(n_components=output_dims, perplexity=50)\n",
    "        reduced_x = tsne.fit_transform(data)\n",
    "    elif method == 'PCA':\n",
    "        pca = PCA(n_components=output_dims)\n",
    "        reduced_x = pca.fit_transform(data)\n",
    "    else:\n",
    "        print(\"Método no disponible %s\" %method)\n",
    "        reduced_x = data\n",
    "    return reduced_x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Entrenar en datos reducidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Train set\n",
    "X = data_train.to_numpy()\n",
    "y = target_train.to_numpy()\n",
    "\n",
    "# Test set\n",
    "X_test = data_test.to_numpy()\n",
    "y_test = target_test.to_numpy()\n",
    "\n",
    "# Metodos\n",
    "methods = ['PCA', 'TSNE']\n",
    "fig1, axes_pred = plt.subplots(1, len(methods), figsize=(10, 3))\n",
    "fig2, axes_correct = plt.subplots(1, len(methods), figsize=(10, 3))\n",
    "\n",
    "for method, ax_pred, ax_correct in zip(methods, axes_pred, axes_correct):\n",
    "    reduced_x = reduce_data(X, method=method, output_dims=2)\n",
    "    print(f\"Reducción de {X.shape} a {reduced_x.shape}\")\n",
    "    clf = LogisticRegression(random_state=0).fit(reduced_x, y)\n",
    "    pred = clf.predict(reduced_x)\n",
    "    error = np.sum(np.abs(y - pred)) / X.shape[0]\n",
    "    print(f\"Reg. logística train, reducción {method}, error de entrenamiento: {error}\")\n",
    "\n",
    "    reduced_x_test = reduce_data(X_test, method=method, output_dims=2)\n",
    "    pred = clf.predict(reduced_x_test)\n",
    "    error = np.mean(np.abs(y_test - pred))\n",
    "    print(f\"Reg. logística test {reduced_x_test.shape}, reducción {method}, error de prueba: {error}\")\n",
    "\n",
    "    # Graficar predicciones del método\n",
    "    for c in range(2):\n",
    "        class_idx = np.where(pred == c)\n",
    "        ax_pred.scatter(\n",
    "            reduced_x_test[class_idx, 0],\n",
    "            reduced_x_test[class_idx,1], \n",
    "            label = f\"class {c}\"\n",
    "        )\n",
    "    \n",
    "    # Graficar en que puntos predijo correctamente la clase\n",
    "    puntos = {\"correcto\":{\n",
    "                    \"indices\": np.where(pred == y_test),\n",
    "                    \"color\": 'green'},\n",
    "             \"incorrecto\":{\n",
    "                    \"indices\": np.where(pred != y_test),\n",
    "                    \"color\": 'red'}\n",
    "             }\n",
    "    for label, info in puntos.items():\n",
    "        ax_correct.scatter(\n",
    "            reduced_x_test[info[\"indices\"], 0],\n",
    "            reduced_x_test[info[\"indices\"],1], \n",
    "            label = label,\n",
    "            color = info[\"color\"]\n",
    "        )\n",
    "\n",
    "    # Visualización de clases\n",
    "    fig1.suptitle(\"Clases\")\n",
    "    ax_pred.set_title(f\"Visualización con {method}\")\n",
    "    ax_pred.legend()\n",
    "\n",
    "    # Visualización de correcto vs incorrecto\n",
    "    fig2.suptitle(\"Clases clasificadas correcta/incorrectamente\")\n",
    "    ax_correct.set_title(f\"Visualización con {method}\")\n",
    "    ax_correct.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Entrenar en alta dimensionalidad y reducir dimensionalidad para visualización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "X = data_train.to_numpy()\n",
    "y = target_train.to_numpy()\n",
    "\n",
    "# Entrenamos un clasificador\n",
    "clf = LogisticRegression(random_state=0).fit(X, y)\n",
    "pred = clf.predict(X)\n",
    "error = np.sum(np.abs(y - pred)) / X.shape[0]\n",
    "print(f\"Error de entrenamiento:\", error)\n",
    "\n",
    "# Nuestros datos son de dimensionalidad 30 osea no podemos visualizar la predicción\n",
    "# Aplicamos reducción de dimensonalidad para visualizar la predicción de nuestro modelo\n",
    "# En el set de prueba\n",
    "X_test = data_test.to_numpy()\n",
    "y_test = target_test.to_numpy()\n",
    "pred = clf.predict(X_test)\n",
    "error = np.mean(np.abs(y_test - pred))\n",
    "print(f\"Reg. logística test {X_test.shape}, error de prueba: {error}\")\n",
    "\n",
    "# Aplicamos reducción de dimensonalidad para visualizar la predicción de nuestro modelo\n",
    "methods = ['PCA', 'TSNE']\n",
    "fig1, axes_pred = plt.subplots(1, len(methods), figsize=(10, 3))\n",
    "fig2, axes_correct = plt.subplots(1, len(methods), figsize=(10, 3))\n",
    "for method, ax_pred, ax_correct in zip(methods, axes_pred, axes_correct):\n",
    "    reduced_x_test = reduce_data(X_test, method=method, output_dims=2)\n",
    "    # Graficar predicciones del método\n",
    "    for c in range(2):\n",
    "        class_idx = np.where(pred == c)\n",
    "        ax_pred.scatter(\n",
    "            reduced_x_test[class_idx, 0],\n",
    "            reduced_x_test[class_idx,1], \n",
    "            label = f\"class {c}\"\n",
    "        )\n",
    "    \n",
    "    # Graficar en que puntos predijo correctamente la clase\n",
    "    puntos = {\"correcto\":{\n",
    "                    \"indices\": np.where(pred == y_test),\n",
    "                    \"color\": 'green'},\n",
    "             \"incorrecto\":{\n",
    "                    \"indices\": np.where(pred != y_test),\n",
    "                    \"color\": 'red'}\n",
    "             }\n",
    "    for label, info in puntos.items():\n",
    "        ax_correct.scatter(\n",
    "            reduced_x_test[info[\"indices\"], 0],\n",
    "            reduced_x_test[info[\"indices\"],1], \n",
    "            label = label,\n",
    "            color = info[\"color\"]\n",
    "        )\n",
    "\n",
    "    # Visualización de clases\n",
    "    fig1.suptitle(\"Clases\")\n",
    "    ax_pred.set_title(f\"Visualización con {method}\")\n",
    "    ax_pred.legend()\n",
    "\n",
    "    # Visualización de correcto vs incorrecto\n",
    "    fig2.suptitle(\"Clases clasificadas correcta/incorrectamente\")\n",
    "    ax_correct.set_title(f\"Visualización con {method}\")\n",
    "    ax_correct.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: En base a lo anterior\n",
    "- ¿Qué puedes concluir?\n",
    "\n",
    "- ¿De que nos sirve reducir la dimensionalidad de los datos?\n",
    "Reducir la dimensionalidad de los datos nos sirve para poder almacenar la mayor cantidad de datos posibles en el menor numero de vectores posibles, asi facilitandonos el uso de datos de gran tamanio, reduciendo tiempos de entrenamiento y haciendo asi nuestro algoritmo mas eficiente.\n",
    "\n",
    "- Para el ejemplo anterior, ¿Qué funciona mejor, entrenar en datos reducidos o en alta dimensionalidad?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    .' \\  (`._   (_)     _   \\\n",
    "  .'    |  '._)         (_)  |\n",
    "  \\ _.')\\      .----..---.   /\n",
    "  |(_.'  |    /    .-\\-.  \\  |\n",
    "  \\     0|    |   ( O| O) | o|\n",
    "   |  _  |  .--.____.'._.-.  |\n",
    "   \\ (_) | o         -` .-`  |\n",
    "    |    \\   |`-._ _ _ _ _\\ /\n",
    "    \\    |   |  `. |_||_|   |\n",
    "    | o  |    \\_      \\     |     -.   .-.\n",
    "    |.-.  \\     `--..-'   O |     `.`-' .'\n",
    "  _.'  .' |     `-.-'      /-.__   ' .-'\n",
    ".' `-.` '.|='=.='=.='=.='=|._/_ `-'.'\n",
    "`-._  `.  |________/\\_____|    `-.'\n",
    "   .'   ).| '=' '='\\/ '=' |\n",
    "   `._.`  '---------------'\n",
    "           //___\\   //___\\\n",
    "             ||       ||\n",
    "     a       ||_.-.   ||_.-.\n",
    "            (_.--__) (_.--__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sistemas_inteligentes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "90e4140b2e60f217aa413489f79b8cb2d030a4bc742884db65b9d8c2b5235a03"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
