{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lP6JLo1tGNBg"
   },
   "source": [
    "# Clasificación con redes neuronales\n",
    "En el ejercicio anterior, creaste y entrenaste una red neuronal para predecir precios de casas utilizando el [Ames Housing Dataset](https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques). Esto consistió en resolver un problema de *regresión*, sin embargo ahora entrenarás una red para resolver un problema de *clasificación*.\n",
    "\n",
    "En este ejercicio utilizarás el [\"Churn Modeling Dataset\"](https://www.kaggle.com/datasets/shivan118/churn-modeling-dataset) para predecir si un cliente cerrará su cuenta bancaria o si continuará siendo cliente de el banco. El dataset contiene información de distintos clientes como su género, su puntuación crediticia, el país de residencia etc. donde para cada cliente se indica si ha dejado el banco (exited = 1) o no (exited = 0).\n",
    "\n",
    "Tu trabajo será poder predecir la probabilidad de que un cliente determinado actual deje el banco.\n",
    "\n",
    "De acuerdo a lo visto en clase responde:\n",
    "1. ¿En el contexto de **clasificación binaria** que función de costo deberías utilizar para entrenar la red?\n",
    "2. ¿Cuál función de activación necesitas en la última capa para predecir una probabilidad de ser clase 1?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gWZyYmS_UE_L"
   },
   "source": [
    "### Importando las librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MxkJoQBkUIHC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hola\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1E0Q3aoKUCRX"
   },
   "source": [
    "## Parte 1 - Preprocesamiento de datos\n",
    "\n",
    "Comenzaremos importando el dataset y visualizando los datos para entender qué necesitamos modificar antes de entrenar el algoritmo.\n",
    "\n",
    "### Importando y visualizando los datos\n",
    "utilizaremos dataset.info() para observar la cantidad de columnas que tenemos, la cantidad de datos y el tipo de dato de cada uno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MXUkhkMfU4wq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 10 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   CreditScore      10000 non-null  int64  \n",
      " 1   Geography        10000 non-null  object \n",
      " 2   Gender           10000 non-null  object \n",
      " 3   Age              10000 non-null  int64  \n",
      " 4   Tenure           10000 non-null  int64  \n",
      " 5   Balance          10000 non-null  float64\n",
      " 6   NumOfProducts    10000 non-null  int64  \n",
      " 7   HasCrCard        10000 non-null  int64  \n",
      " 8   IsActiveMember   10000 non-null  int64  \n",
      " 9   EstimatedSalary  10000 non-null  float64\n",
      "dtypes: float64(2), int64(6), object(2)\n",
      "memory usage: 781.4+ KB\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv('Churn_Modelling.csv')\n",
    "# Las primeras tres columnas no importan para la predicción por lo que las ignoraremos\n",
    "full_data = dataset.iloc[:, 3:-1]\n",
    "full_labels = dataset.iloc[:, -1].values\n",
    "\n",
    "full_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En base a la información anterior responde:\n",
    "1. ¿Cuantas variables de entrada tiene cada datapoint del dataset?\n",
    "2. ¿Cuantos datos tiene el dataset?\n",
    "3. ¿Cuantas y cuales columnas **no** son de tipo numérico?\n",
    "\n",
    "Ejecuta la siguiente linea para visualizar algunos datos de entrenamiento junto con sus etiquetas. Esto nos ayudará a entender qué tipo de pre procesamiento necesitamos antes de entrenar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Datos\", full_data.head(3))\n",
    "#print(\"Etiquetas\", full_labels[:3])\n",
    "\n",
    "\n",
    "# TODO: Define una lista con los nombres de las columnas con valores categóricos\n",
    "\n",
    "object_columns = [\"Geography\", \"Gender\",\"Age\",\"Tenure\",\"Balance\",\"NumOfProducts\",\"HasCrCard\",\"IsActiveMember\",\"EstimatedSalary\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vHol938cW8zd"
   },
   "source": [
    "### Dividiendo los datos en entrenamiento y validación\n",
    "Notarás que este dataset solo te dá los datos de entrenamiento, por lo que los dividiremos en entrenamiento y validación.\n",
    "\n",
    "En este caso utilizaremos la utilería de sklearn [`train_test_split`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) para realizar la divición."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z-TDt0Y_XEfc"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "test_size = 0.2\n",
    "# TODO: divide los datos en entrenamiento y validación,\n",
    "# asignando el 20% de los datos a validación/prueba\n",
    "X_train, X_val, y_train, y_val = train_test_split(full_data, full_labels, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N6bQ0UgSU-NJ"
   },
   "source": [
    "### Codificando datos categóricos\n",
    "Al igual que en el problema de las casas, podrás notar que algunas columnas son de tipo string mientras que la red neuronal necesita valores numéricos para poder entrenar. Por lo tanto aplicaremos un pre procesamiento similar al ejercicio anterior.\n",
    "\n",
    "Utiliza la clase [OrdinalEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html) de sklearn para transformar todas las columnas categóricas a un valor entero. Recuerda que esta clase espera que los datos ingresados sean del tipo.\n",
    "\n",
    "En este dataset, además de transformar los valores categóricos a enteros, adicionalmente normalizaremos los datos de entrada utilizando la clase [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) utilizada en el ejercicio de PCA. \n",
    "\n",
    "De esta forma, nuestro procesamiento de datos se verá de la siguiente forma:\n",
    "1. Ajustar el codificador de columnas categóricas a los datos de entrenamiento\n",
    "2. Ajustar el normalizador a los datos de entrenamiento\n",
    "3. Aplicar el codificador al dataset indicado (train/val/test)\n",
    "4. Remover los valores NaN remplazándolos por -1\n",
    "5. Aplicar el normalizador al dataset indicado (train/val/test)\n",
    "\n",
    "En la siguiente celda, completa el código faltante para preprocesar los datos. Recuerda que anteriormente has definido las columnas de tipo categórico. Utiliza la variable object_columns para aplicar el OrdinalEncoder a las columnas categóricas de tus datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PxVKWXxLbczC"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# TODO: Define y ajusta un codificador (OrdinalEncoder)\n",
    "# para transformar las columnas categóricas a valores numéricos\n",
    "# usando los datos de entrenamiento (X_train)\n",
    "feat_encoder = OrdinalEncoder(handle_unknown='use_encoded_value',\n",
    "                              unknown_value=-1)\n",
    "feat_encoder.fit(X_train[object_columns])\n",
    "\n",
    "# Transformando categórigos a numéricos\n",
    "X_transformed = X_train.copy()\n",
    "X_transformed[object_columns] = feat_encoder.transform(X_train[object_columns])\n",
    "\n",
    "# TODO: Define y ajusta un normalizador (StandardScaler)\n",
    "# con los datos de entrenamiento transformados (X_transformed)\n",
    "normalizer = StandardScaler()\n",
    "normalizer.fit(X_transformed)\n",
    "\n",
    "# TODO: Completa el método apply_preprocessing\n",
    "# para aplicar la misma codificación a cualquier dataset\n",
    "def apply_preprocessing(dataset, feat_encoder, normalizer, obj_cols):\n",
    "    '''\n",
    "        args:\n",
    "        - dataset (pd.DataFrame): Conjunto de datos\n",
    "        - feat_encoder (OrdinalEncoder): instancia de codificador para las variables de entrada ajustado con datos de entrenamiento\n",
    "        returns:\n",
    "        - transformed_dataset (np.array): dataset transformado\n",
    "    '''\n",
    "    transformed_dataset = dataset.copy()\n",
    "    # TODO: utiliza feat_encoder para transformar los valores categóricos del dataset a enteros.\n",
    "    transformed_dataset[obj_cols] = feat_encoder.transform(dataset[obj_cols])\n",
    "\n",
    "    # TODO: utiliza normalizer para normalizar los datos transformados\n",
    "    transformed_dataset = normalizer.transform(transformed_dataset)\n",
    "\n",
    "    # Reemplazando valores NaN con -1\n",
    "    transformed_dataset[np.isnan(transformed_dataset)] = -1\n",
    "    return transformed_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que has terminado de definir el pre procesamiento de datos, aplícalo a los dos conjuntos de datos (entrenamiento y validación)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenamiento shapes (8000, 10) (8000,)\n",
      "Entrenamiento pre procesado [[ 0.16958176  1.51919821 -1.09168714]\n",
      " [-2.30455945  0.3131264   0.91601335]\n",
      " [-1.19119591 -0.89294542 -1.09168714]\n",
      " ...\n",
      " [ 0.9015152  -0.89294542  0.91601335]\n",
      " [-0.62420521  1.51919821 -1.09168714]\n",
      " [-0.28401079  0.3131264  -1.09168714]]\n",
      "Validacion shapes (2000, 10) (2000,)\n",
      "Validacion pre procesado [[-0.55204276  0.3131264  -1.09168714]\n",
      " [-1.31490297 -0.89294542 -1.09168714]\n",
      " [ 0.57162971  1.51919821 -1.09168714]\n",
      " ...\n",
      " [-0.74791227  1.51919821  0.91601335]\n",
      " [-0.00566991  0.3131264   0.91601335]\n",
      " [-0.79945688  0.3131264   0.91601335]]\n"
     ]
    }
   ],
   "source": [
    "X_train = apply_preprocessing(X_train, feat_encoder, normalizer, object_columns)\n",
    "print(\"Entrenamiento shapes\", X_train.shape, y_train.shape)\n",
    "print(\"Entrenamiento pre procesado\", X_train[:,:3])\n",
    "\n",
    "# TODO: Aplica el pre procesamiento de datos a los datos de validación\n",
    "X_val = apply_preprocessing(X_val, feat_encoder, normalizer, object_columns)\n",
    "print(\"Validacion shapes\", X_val.shape, y_val.shape)\n",
    "print(\"Validacion pre procesado\", X_val[:,:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creando los dataloader\n",
    "Ahora que hemos limpiado los datos podemos crear  los data loaders para entrenamiento y validación. Ejecuta la siguiente celda para ello."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "class BankDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data.astype('float32')\n",
    "        self.labels = labels.astype('float32')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        datapoint = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        label = np.expand_dims(label, 0) # Transformarlo a vector de 1x1\n",
    "        return datapoint, label\n",
    "\n",
    "# Datasets\n",
    "train_dataset = BankDataset(X_train, y_train)\n",
    "val_dataset = BankDataset(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-zfEzkRVXIwF"
   },
   "source": [
    "## Parte 2 - Construyendo la red\n",
    "Es momento de crear tu red! puedes basarte en el ejercicio anterior para definirla. En la siguiente celda, define tu red neuronal. Al ser este un problema de clasificación, recuerda seleccionar una función de activación apropiada para la última capa. Puedes referirte a la documentación de pytorch sobre [funciones de activación](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KvdeScabXtlB"
   },
   "source": [
    "### Define tu red neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3dtrScHxXQox"
   },
   "outputs": [],
   "source": [
    "# TODO: Calcula las variables de entrada y salida para algun punto de X_train\n",
    "input_dims = X_train.shape[1]\n",
    "output_dims = 1\n",
    "\n",
    "# TODO: Define la red neuronal\n",
    "model =  nn.Sequential(\n",
    "    nn.Linear(input_dims, 90),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(90, 90),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(90, output_dims),\n",
    "    nn.Sigmoid()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JT4u2S1_Y4WG"
   },
   "source": [
    "## Part 3 - Entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8GWlJChhY_ZI"
   },
   "source": [
    "### Declarando hiperparámetros y optimizadores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fG3RrwDXZEaS"
   },
   "outputs": [],
   "source": [
    "# hiperparametros\n",
    "learning_rate = 1e-3\n",
    "batch_size = 128\n",
    "epochs = 200\n",
    "\n",
    "# Declaramos el optimizador\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# TODO: Declara una función de costo apropiada para clasificación\n",
    "# https://pytorch.org/docs/stable/nn.html#loss-functions\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "# Data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0QR_G5u7ZLSM"
   },
   "source": [
    "### Entrenando la red en datos de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 33685,
     "status": "ok",
     "timestamp": 1590257481284,
     "user": {
      "displayName": "Hadelin de Ponteves",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhEuXdT7eQweUmRPW8_laJuPggSK6hfvpl5a6WBaA=s64",
      "userId": "15047218817161520419"
     },
     "user_tz": -240
    },
    "id": "nHZ-LKv_ZRb3",
    "outputId": "718cc4b0-b5aa-40f0-9b20-d3d31730a531"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 \t train loss: 0.17151427375418798 \t val_loss: 0.5873193126171827\n",
      "epoch: 1 \t train loss: 0.1737176730992302 \t val_loss: 0.6057861391454935\n",
      "epoch: 2 \t train loss: 0.17130928782243576 \t val_loss: 0.5989613570272923\n",
      "epoch: 3 \t train loss: 0.16986147550836442 \t val_loss: 0.5967206060886383\n",
      "epoch: 4 \t train loss: 0.17096912754433496 \t val_loss: 0.652362622320652\n",
      "epoch: 5 \t train loss: 0.17168738255425106 \t val_loss: 0.612932737916708\n",
      "epoch: 6 \t train loss: 0.17070538917231182 \t val_loss: 0.6042720824480057\n",
      "epoch: 7 \t train loss: 0.16823320138075995 \t val_loss: 0.6097655799239874\n",
      "epoch: 8 \t train loss: 0.16785382633171383 \t val_loss: 0.6065180543810129\n",
      "epoch: 9 \t train loss: 0.165902836336976 \t val_loss: 0.5914992522448301\n",
      "epoch: 10 \t train loss: 0.16720218187759792 \t val_loss: 0.5954820923507214\n",
      "epoch: 11 \t train loss: 0.16390344760720693 \t val_loss: 0.6385333798825741\n",
      "epoch: 12 \t train loss: 0.16564379147593938 \t val_loss: 0.6061877049505711\n",
      "epoch: 13 \t train loss: 0.16357867727203976 \t val_loss: 0.6161964293569326\n",
      "epoch: 14 \t train loss: 0.163950743774573 \t val_loss: 0.5971313565969467\n",
      "epoch: 15 \t train loss: 0.16188099802959532 \t val_loss: 0.6161163952201605\n",
      "epoch: 16 \t train loss: 0.16183124364368498 \t val_loss: 0.6167602762579918\n",
      "epoch: 17 \t train loss: 0.16401719290112693 \t val_loss: 0.6230021547526121\n",
      "epoch: 18 \t train loss: 0.16339770964686834 \t val_loss: 0.612311216071248\n",
      "epoch: 19 \t train loss: 0.16193474438928424 \t val_loss: 0.6217777263373137\n",
      "epoch: 20 \t train loss: 0.16216904090510476 \t val_loss: 0.6241332869976759\n",
      "epoch: 21 \t train loss: 0.1617973276547023 \t val_loss: 0.6147027239203453\n",
      "epoch: 22 \t train loss: 0.16144859246791354 \t val_loss: 0.6493100319057703\n",
      "epoch: 23 \t train loss: 0.16052538085551488 \t val_loss: 0.6448125094175339\n",
      "epoch: 24 \t train loss: 0.15767368035657064 \t val_loss: 0.6396454703062773\n",
      "epoch: 25 \t train loss: 0.15786626284557675 \t val_loss: 0.6325937323272228\n",
      "epoch: 26 \t train loss: 0.1568615104234408 \t val_loss: 0.6309024803340435\n",
      "epoch: 27 \t train loss: 0.15669056678575183 \t val_loss: 0.6800628304481506\n",
      "epoch: 28 \t train loss: 0.15903963361467635 \t val_loss: 0.6547535825520754\n",
      "epoch: 29 \t train loss: 0.15785107418658242 \t val_loss: 0.631848031654954\n",
      "epoch: 30 \t train loss: 0.15720898007589673 \t val_loss: 0.667694553732872\n",
      "epoch: 31 \t train loss: 0.15445024449200856 \t val_loss: 0.6191628500819206\n",
      "epoch: 32 \t train loss: 0.15352751373771636 \t val_loss: 0.6327448952943087\n",
      "epoch: 33 \t train loss: 0.15370514125577986 \t val_loss: 0.6506166309118271\n",
      "epoch: 34 \t train loss: 0.15232860829148973 \t val_loss: 0.672591270878911\n",
      "epoch: 35 \t train loss: 0.15448620383228576 \t val_loss: 0.6817055884748697\n",
      "epoch: 36 \t train loss: 0.15661652090530548 \t val_loss: 0.635355269536376\n",
      "epoch: 37 \t train loss: 0.15107465689144436 \t val_loss: 0.6407833825796843\n",
      "epoch: 38 \t train loss: 0.15052169667822973 \t val_loss: 0.7120699342340231\n",
      "epoch: 39 \t train loss: 0.1522710540937999 \t val_loss: 0.6500132903456688\n",
      "epoch: 40 \t train loss: 0.14893946642913516 \t val_loss: 0.6486606877297163\n",
      "epoch: 41 \t train loss: 0.1548382281547501 \t val_loss: 0.6692078560590744\n",
      "epoch: 42 \t train loss: 0.15254789508051342 \t val_loss: 0.649213956668973\n",
      "epoch: 43 \t train loss: 0.14746107243829304 \t val_loss: 0.6566026471555233\n",
      "epoch: 44 \t train loss: 0.1477862074971199 \t val_loss: 0.6555410586297512\n",
      "epoch: 45 \t train loss: 0.149713376448268 \t val_loss: 0.6973989680409431\n",
      "epoch: 46 \t train loss: 0.1483868152376205 \t val_loss: 0.6751188021153212\n",
      "epoch: 47 \t train loss: 0.14816758036613464 \t val_loss: 0.6443581450730562\n",
      "epoch: 48 \t train loss: 0.14880575771842683 \t val_loss: 0.666455589234829\n",
      "epoch: 49 \t train loss: 0.14544984591858728 \t val_loss: 0.6941726002842188\n",
      "epoch: 50 \t train loss: 0.14521537697504436 \t val_loss: 0.7026501875370741\n",
      "epoch: 51 \t train loss: 0.1444979721591586 \t val_loss: 0.704995509237051\n",
      "epoch: 52 \t train loss: 0.15083226395977867 \t val_loss: 0.7052056137472391\n",
      "epoch: 53 \t train loss: 0.14211478947647035 \t val_loss: 0.675578635185957\n",
      "epoch: 54 \t train loss: 0.1447154903222644 \t val_loss: 0.6940945591777563\n",
      "epoch: 55 \t train loss: 0.14261439513592494 \t val_loss: 0.679990828037262\n",
      "epoch: 56 \t train loss: 0.14211683276863324 \t val_loss: 0.6934162173420191\n",
      "epoch: 57 \t train loss: 0.13987179325213508 \t val_loss: 0.689640711992979\n",
      "epoch: 58 \t train loss: 0.14084333950091923 \t val_loss: 0.690036416053772\n",
      "epoch: 59 \t train loss: 0.14607934464537908 \t val_loss: 0.712768966332078\n",
      "epoch: 60 \t train loss: 0.14465073803587566 \t val_loss: 0.7137745562940836\n",
      "epoch: 61 \t train loss: 0.1392535250338297 \t val_loss: 0.7064271159470081\n",
      "epoch: 62 \t train loss: 0.13965668334137826 \t val_loss: 0.7015047427266836\n",
      "epoch: 63 \t train loss: 0.13998018772829146 \t val_loss: 0.69535699672997\n",
      "epoch: 64 \t train loss: 0.1382145607282245 \t val_loss: 0.6990114953368902\n",
      "epoch: 65 \t train loss: 0.138316447772677 \t val_loss: 0.6851559337228537\n",
      "epoch: 66 \t train loss: 0.13651038114986722 \t val_loss: 0.6998583786189556\n",
      "epoch: 67 \t train loss: 0.1371058245736455 \t val_loss: 0.688719866797328\n",
      "epoch: 68 \t train loss: 0.14371117381822496 \t val_loss: 0.700882725417614\n",
      "epoch: 69 \t train loss: 0.13503040006709477 \t val_loss: 0.7161545250564814\n",
      "epoch: 70 \t train loss: 0.13462053227519233 \t val_loss: 0.7329076062887907\n",
      "epoch: 71 \t train loss: 0.13157644773286487 \t val_loss: 0.7214527204632759\n",
      "epoch: 72 \t train loss: 0.13543439443622315 \t val_loss: 0.7140021976083517\n",
      "epoch: 73 \t train loss: 0.1369597716700463 \t val_loss: 0.7232149671763182\n",
      "epoch: 74 \t train loss: 0.13504132320956577 \t val_loss: 0.7401783969253302\n",
      "epoch: 75 \t train loss: 0.13029011563649254 \t val_loss: 0.7325953282415867\n",
      "epoch: 76 \t train loss: 0.1381985190368834 \t val_loss: 0.7108635846525431\n",
      "epoch: 77 \t train loss: 0.13418683031248668 \t val_loss: 0.7190226390957832\n",
      "epoch: 78 \t train loss: 0.1306076357288966 \t val_loss: 0.736882172524929\n",
      "epoch: 79 \t train loss: 0.13038405468539585 \t val_loss: 0.7206981964409351\n",
      "epoch: 80 \t train loss: 0.1338246012963946 \t val_loss: 0.7194317076355219\n",
      "epoch: 81 \t train loss: 0.12860525257530667 \t val_loss: 0.740250039845705\n",
      "epoch: 82 \t train loss: 0.13147190142245518 \t val_loss: 0.7379676904529333\n",
      "epoch: 83 \t train loss: 0.1377402823122721 \t val_loss: 0.7383391782641411\n",
      "epoch: 84 \t train loss: 0.13061667623974027 \t val_loss: 0.759501326829195\n",
      "epoch: 85 \t train loss: 0.12809045293501445 \t val_loss: 0.729395804926753\n",
      "epoch: 86 \t train loss: 0.12854336533281538 \t val_loss: 0.7390210814774036\n",
      "epoch: 87 \t train loss: 0.13086774170635238 \t val_loss: 0.7396996654570103\n",
      "epoch: 88 \t train loss: 0.12732979903618494 \t val_loss: 0.7583745066076517\n",
      "epoch: 89 \t train loss: 0.12456054524296806 \t val_loss: 0.745011093094945\n",
      "epoch: 90 \t train loss: 0.12509397594701677 \t val_loss: 0.7436028383672237\n",
      "epoch: 91 \t train loss: 0.12883434562929094 \t val_loss: 0.7436037566512823\n",
      "epoch: 92 \t train loss: 0.12552286755471004 \t val_loss: 0.727422371506691\n",
      "epoch: 93 \t train loss: 0.12287590723662149 \t val_loss: 0.7598645705729723\n",
      "epoch: 94 \t train loss: 0.12518070224258634 \t val_loss: 0.765728322789073\n",
      "epoch: 95 \t train loss: 0.12144870772248223 \t val_loss: 0.7734347023069859\n",
      "epoch: 96 \t train loss: 0.12426452445132392 \t val_loss: 0.7640575207769871\n",
      "epoch: 97 \t train loss: 0.12515013866008273 \t val_loss: 0.752195343375206\n",
      "epoch: 98 \t train loss: 0.12509316643552174 \t val_loss: 0.7762834783643484\n",
      "epoch: 99 \t train loss: 0.1220439158025242 \t val_loss: 0.7563401758670807\n",
      "epoch: 100 \t train loss: 0.1241453042815602 \t val_loss: 0.7750663198530674\n",
      "epoch: 101 \t train loss: 0.11840808308786815 \t val_loss: 0.788707485422492\n",
      "epoch: 102 \t train loss: 0.11896220440902407 \t val_loss: 0.7581790685653687\n",
      "epoch: 103 \t train loss: 0.11826396000290675 \t val_loss: 0.7863043006509542\n",
      "epoch: 104 \t train loss: 0.1259889453649521 \t val_loss: 0.7741059772670269\n",
      "epoch: 105 \t train loss: 0.11914780121000987 \t val_loss: 0.765410304069519\n",
      "epoch: 106 \t train loss: 0.12084120216350706 \t val_loss: 0.7769629284739494\n",
      "epoch: 107 \t train loss: 0.12047942171967219 \t val_loss: 0.770128658041358\n",
      "epoch: 108 \t train loss: 0.11665176286820382 \t val_loss: 0.8176319375634193\n",
      "epoch: 109 \t train loss: 0.11795822933079704 \t val_loss: 0.8099176287651062\n",
      "epoch: 110 \t train loss: 0.11915951807584081 \t val_loss: 0.7710023336112499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 111 \t train loss: 0.11659845366837486 \t val_loss: 0.7830343600362539\n",
      "epoch: 112 \t train loss: 0.11438863186372651 \t val_loss: 0.816493920981884\n",
      "epoch: 113 \t train loss: 0.11460697402556737 \t val_loss: 0.8060064241290092\n",
      "epoch: 114 \t train loss: 0.11443391873959512 \t val_loss: 0.7796433996409178\n",
      "epoch: 115 \t train loss: 0.11601528998405214 \t val_loss: 0.8071021251380444\n",
      "epoch: 116 \t train loss: 0.11690290828072836 \t val_loss: 0.8788014799356461\n",
      "epoch: 117 \t train loss: 0.11611546173928276 \t val_loss: 0.8021009638905525\n",
      "epoch: 118 \t train loss: 0.11421892336673206 \t val_loss: 0.803643960505724\n",
      "epoch: 119 \t train loss: 0.11117077057087232 \t val_loss: 0.7866020686924458\n",
      "epoch: 120 \t train loss: 0.11228065240004706 \t val_loss: 0.8000119868665934\n",
      "epoch: 121 \t train loss: 0.11071606518493758 \t val_loss: 0.8599508814513683\n",
      "epoch: 122 \t train loss: 0.11223715212610033 \t val_loss: 0.7935495469719172\n",
      "epoch: 123 \t train loss: 0.10746842096485788 \t val_loss: 0.8032812085002661\n",
      "epoch: 124 \t train loss: 0.10854714600339768 \t val_loss: 0.8266535997390747\n",
      "epoch: 125 \t train loss: 0.11071818239159054 \t val_loss: 0.8420522641390562\n",
      "epoch: 126 \t train loss: 0.11191007423968542 \t val_loss: 0.8332068733870983\n",
      "epoch: 127 \t train loss: 0.11068076977417582 \t val_loss: 0.8119251914322376\n",
      "epoch: 128 \t train loss: 0.11002108407399011 \t val_loss: 0.8398437425494194\n",
      "epoch: 129 \t train loss: 0.1149926877447537 \t val_loss: 0.8395626209676266\n",
      "epoch: 130 \t train loss: 0.10588039304055864 \t val_loss: 0.8331652469933033\n",
      "epoch: 131 \t train loss: 0.10566648941427942 \t val_loss: 0.8432100117206573\n",
      "epoch: 132 \t train loss: 0.10817454317732463 \t val_loss: 0.844721831381321\n",
      "epoch: 133 \t train loss: 0.10854061314510921 \t val_loss: 0.8259171061217785\n",
      "epoch: 134 \t train loss: 0.10471469364942067 \t val_loss: 0.8450557310134172\n",
      "epoch: 135 \t train loss: 0.108884989564854 \t val_loss: 0.8415215872228146\n",
      "epoch: 136 \t train loss: 0.10637254055057253 \t val_loss: 0.8674724958837032\n",
      "epoch: 137 \t train loss: 0.10405262837570811 \t val_loss: 0.8481168560683727\n",
      "epoch: 138 \t train loss: 0.10689618450308604 \t val_loss: 0.8419159650802612\n",
      "epoch: 139 \t train loss: 0.10725164969289114 \t val_loss: 0.8438010662794113\n",
      "epoch: 140 \t train loss: 0.10796546214629733 \t val_loss: 0.8569709435105324\n",
      "epoch: 141 \t train loss: 0.10136694922333672 \t val_loss: 0.8468653336167336\n",
      "epoch: 142 \t train loss: 0.10353760687368256 \t val_loss: 0.8614982813596725\n",
      "epoch: 143 \t train loss: 0.10248705715177552 \t val_loss: 0.8487923182547092\n",
      "epoch: 144 \t train loss: 0.10104320440737027 \t val_loss: 0.8990825228393078\n",
      "epoch: 145 \t train loss: 0.09995987491002159 \t val_loss: 0.8728254325687885\n",
      "epoch: 146 \t train loss: 0.1018743198660631 \t val_loss: 0.8957028612494469\n",
      "epoch: 147 \t train loss: 0.10166117217805651 \t val_loss: 0.859723599627614\n",
      "epoch: 148 \t train loss: 0.10522176414018586 \t val_loss: 0.844038711860776\n",
      "epoch: 149 \t train loss: 0.09838700933115822 \t val_loss: 0.8512206897139549\n",
      "epoch: 150 \t train loss: 0.0983107876446512 \t val_loss: 0.8790583200752735\n",
      "epoch: 151 \t train loss: 0.10166634506885967 \t val_loss: 0.8700392618775368\n",
      "epoch: 152 \t train loss: 0.09969264398964625 \t val_loss: 0.8880327828228474\n",
      "epoch: 153 \t train loss: 0.10237321411333387 \t val_loss: 0.8878180757164955\n",
      "epoch: 154 \t train loss: 0.10056186745327617 \t val_loss: 0.8719766326248646\n",
      "epoch: 155 \t train loss: 0.0975472814743481 \t val_loss: 0.8897374346852303\n",
      "epoch: 156 \t train loss: 0.0976589774446828 \t val_loss: 0.8701989986002445\n",
      "epoch: 157 \t train loss: 0.09461637870186851 \t val_loss: 0.8758785240352154\n",
      "epoch: 158 \t train loss: 0.09964034416609341 \t val_loss: 0.8734190613031387\n",
      "epoch: 159 \t train loss: 0.09914815857533425 \t val_loss: 0.9042294956743717\n",
      "epoch: 160 \t train loss: 0.09478154228556723 \t val_loss: 0.9297395795583725\n",
      "epoch: 161 \t train loss: 0.09220113534302939 \t val_loss: 0.8991711847484112\n",
      "epoch: 162 \t train loss: 0.09524121342433824 \t val_loss: 0.9230988845229149\n",
      "epoch: 163 \t train loss: 0.09379096861396517 \t val_loss: 0.9228671044111252\n",
      "epoch: 164 \t train loss: 0.094317572280055 \t val_loss: 0.9256591498851776\n",
      "epoch: 165 \t train loss: 0.09554477804709995 \t val_loss: 0.9237851612269878\n",
      "epoch: 166 \t train loss: 0.09308311544240468 \t val_loss: 0.9225567989051342\n",
      "epoch: 167 \t train loss: 0.09154923922485775 \t val_loss: 0.9075879603624344\n",
      "epoch: 168 \t train loss: 0.09022702802977865 \t val_loss: 0.9059200324118137\n",
      "epoch: 169 \t train loss: 0.09749045329434532 \t val_loss: 0.9237599521875381\n",
      "epoch: 170 \t train loss: 0.09563918683737044 \t val_loss: 0.9765245914459229\n",
      "epoch: 171 \t train loss: 0.09519517208848681 \t val_loss: 0.9528826549649239\n",
      "epoch: 172 \t train loss: 0.09313095372820658 \t val_loss: 0.9280781634151936\n",
      "epoch: 173 \t train loss: 0.08952493664054643 \t val_loss: 0.958811093121767\n",
      "epoch: 174 \t train loss: 0.08789791923666758 \t val_loss: 0.9373625740408897\n",
      "epoch: 175 \t train loss: 0.08896865352751716 \t val_loss: 0.9135353043675423\n",
      "epoch: 176 \t train loss: 0.09258405178312272 \t val_loss: 0.9209585562348366\n",
      "epoch: 177 \t train loss: 0.08853793274315577 \t val_loss: 0.9599871523678303\n",
      "epoch: 178 \t train loss: 0.08805702195044547 \t val_loss: 0.986583549529314\n",
      "epoch: 179 \t train loss: 0.08703271961874431 \t val_loss: 0.9193202294409275\n",
      "epoch: 180 \t train loss: 0.0882904048240374 \t val_loss: 0.9401235207915306\n",
      "epoch: 181 \t train loss: 0.0915519024526316 \t val_loss: 0.9534199126064777\n",
      "epoch: 182 \t train loss: 0.09047718304726812 \t val_loss: 0.9541453421115875\n",
      "epoch: 183 \t train loss: 0.08750792373977011 \t val_loss: 0.9281718507409096\n",
      "epoch: 184 \t train loss: 0.08783654732600091 \t val_loss: 0.9406437501311302\n",
      "epoch: 185 \t train loss: 0.08538852501956243 \t val_loss: 0.9471012465655804\n",
      "epoch: 186 \t train loss: 0.08471047026770455 \t val_loss: 0.990756057202816\n",
      "epoch: 187 \t train loss: 0.08363263681530952 \t val_loss: 0.9813933074474335\n",
      "epoch: 188 \t train loss: 0.08874636143445969 \t val_loss: 0.9459654577076435\n",
      "epoch: 189 \t train loss: 0.08371158801610508 \t val_loss: 1.0092306844890118\n",
      "epoch: 190 \t train loss: 0.08813261146110202 \t val_loss: 0.9535077922046185\n",
      "epoch: 191 \t train loss: 0.09220108880646645 \t val_loss: 0.9733797945082188\n",
      "epoch: 192 \t train loss: 0.08363793754861468 \t val_loss: 0.9692762047052383\n",
      "epoch: 193 \t train loss: 0.08252531644843873 \t val_loss: 0.9814887642860413\n",
      "epoch: 194 \t train loss: 0.08161505087027474 \t val_loss: 0.9994277283549309\n",
      "epoch: 195 \t train loss: 0.0814895105976907 \t val_loss: 0.9834557846188545\n",
      "epoch: 196 \t train loss: 0.08250230598071265 \t val_loss: 0.9744953252375126\n",
      "epoch: 197 \t train loss: 0.08334128836554194 \t val_loss: 0.9737905524671078\n",
      "epoch: 198 \t train loss: 0.084404324018766 \t val_loss: 0.9804562591016293\n",
      "epoch: 199 \t train loss: 0.08135015884089092 \t val_loss: 1.007236286997795\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    losses = []\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        # Aplicando descenso de gradiente \n",
    "        # ======== Start ============\n",
    "        # TODO: calcula las predicciones del modelo para x_batch\n",
    "        predictions = model(x_batch)\n",
    "\n",
    "        # TODO: Calcula el costo de las predictiones, contra las etiquetas y_batch\n",
    "        loss = loss_fn(predictions, y_batch)\n",
    "\n",
    "        # TODO: Calcula los gradientes\n",
    "        loss.backward()\n",
    "\n",
    "        # TODO: Actualiza los pesos\n",
    "        optimizer.step()\n",
    "        # ======== End ============\n",
    "        losses.append(loss.item())\n",
    "    # Validación\n",
    "    val_loss = 0.0\n",
    "    for i, data in enumerate(val_loader, 0):\n",
    "        inputs, labels = data\n",
    "        with torch.inference_mode():\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "    val_loss = val_loss/len(val_loader)\n",
    "    print(f\"epoch: {epoch} \\t train loss: {np.mean(losses)} \\t val_loss: {val_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tJj5k2MxZga3"
   },
   "source": [
    "## Part 4 - Evaluación del modelo\n",
    "Ahora que has entrenado tu modelo, vamos a utilizarlo para evaluarlo en un nuevo punto nunca antes visto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "84QFoqGYeXHL"
   },
   "source": [
    "### Predice el resultado para la siguiente observación:\n",
    "Utiliza tu modelo para predecir si el siguiente cliente dejará el banco:\n",
    "- CreditScore: 600\n",
    "- Geography: \"France\"\n",
    "- Gender: \"Male\"\n",
    "- Tenure: 3\n",
    "- Balance: 60000\n",
    "- NumOfProducts: 2\n",
    "- HasCrCard: 1\n",
    "- IsActiveMember: 1\n",
    "- EstimatedSalary: 50000\n",
    "\n",
    "¿Deberíamos despedirnos de este cliente?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZhU1LTgPg-kH"
   },
   "source": [
    "**Solution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 33990,
     "status": "ok",
     "timestamp": 1590257481594,
     "user": {
      "displayName": "Hadelin de Ponteves",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhEuXdT7eQweUmRPW8_laJuPggSK6hfvpl5a6WBaA=s64",
      "userId": "15047218817161520419"
     },
     "user_tz": -240
    },
    "id": "2d8IoCCkeWGL",
    "outputId": "957f3970-e197-4c3b-a150-7f69dc567f5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.]], grad_fn=<SigmoidBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robertoibarra/opt/anaconda3/envs/sistemas_inteligentes/lib/python3.8/site-packages/sklearn/base.py:420: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "datapoint = np.array([600, 0, 1, 40, 3, 60000, 2, 1, 1, 50000])\n",
    "datapoint=datapoint.reshape(1, -1)\n",
    "datapoint = normalizer.transform(datapoint)\n",
    "inp_tensor = torch.from_numpy(datapoint).float()\n",
    "\n",
    "output= model(inp_tensor)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wGjx94g2n7OV"
   },
   "source": [
    "Therefore, our ANN model predicts that this customer stays in the bank!\n",
    "\n",
    "**Important note 1:** Notice that the values of the features were all input in a double pair of square brackets. That's because the \"predict\" method always expects a 2D array as the format of its inputs. And putting our values into a double pair of square brackets makes the input exactly a 2D array.\n",
    "\n",
    "**Important note 2:** Notice also that the \"France\" country was not input as a string in the last column but as \"1, 0, 0\" in the first three columns. That's because of course the predict method expects the one-hot-encoded values of the state, and as we see in the first row of the matrix of features X, \"France\" was encoded as \"1, 0, 0\". And be careful to include these values in the first three columns, because the dummy variables are always created in the first columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u7yx47jPZt11"
   },
   "source": [
    "### Predicting the Test set results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 137
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 33987,
     "status": "ok",
     "timestamp": 1590257481595,
     "user": {
      "displayName": "Hadelin de Ponteves",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhEuXdT7eQweUmRPW8_laJuPggSK6hfvpl5a6WBaA=s64",
      "userId": "15047218817161520419"
     },
     "user_tz": -240
    },
    "id": "nIyEeQdRZwgs",
    "outputId": "82330ba8-9bdc-4fd1-d3cf-b6d78ee7c2a3"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m X_test_torch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(\u001b[43mX_test\u001b[49m)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m      2\u001b[0m y_test_torch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(y_test)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m      4\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model(X_test_torch)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_test' is not defined"
     ]
    }
   ],
   "source": [
    "X_test_torch = torch.from_numpy(X_test).float()\n",
    "y_test_torch = torch.from_numpy(y_test).float()\n",
    "\n",
    "y_pred = model(X_test_torch)\n",
    "y_pred = (y_pred > 0.5).numpy()\n",
    "print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o0oyfLWoaEGw"
   },
   "source": [
    "### Making the Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 33981,
     "status": "ok",
     "timestamp": 1590257481595,
     "user": {
      "displayName": "Hadelin de Ponteves",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhEuXdT7eQweUmRPW8_laJuPggSK6hfvpl5a6WBaA=s64",
      "userId": "15047218817161520419"
     },
     "user_tz": -240
    },
    "id": "ci6K_r6LaF6P",
    "outputId": "4d854e9e-22d5-432f-f6e5-a102fe3ae0bd"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m confusion_matrix, accuracy_score\n\u001b[0;32m----> 2\u001b[0m cm \u001b[38;5;241m=\u001b[39m confusion_matrix(\u001b[43my_test\u001b[49m, y_pred)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(cm)\n\u001b[1;32m      4\u001b[0m accuracy_score(y_test, y_pred)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_test' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMeRFWFoGrdaL5S3dx5MWmb",
   "collapsed_sections": [],
   "name": "artificial_neural_network.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
